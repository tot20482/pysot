import os
import json
import random
import math
import time
import logging
from tqdm import tqdm

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import argparse

from pysot.utils.lr_scheduler import build_lr_scheduler
from torch.utils.data.distributed import DistributedSampler

from pysot.utils.distributed import get_rank, get_world_size, reduce_gradients, average_reduce, dist_init, DistModule
from pysot.utils.model_load import load_pretrain, restore_from
from pysot.utils.average_meter import AverageMeter
from pysot.models.model_builder import ModelBuilder
from pysot.utils.distributed import is_dist_avail_and_initialized
from pysot.core.config import cfg
from tensorboardX import SummaryWriter


# -------------------- Dataset --------------------
class ProcessedDataset(Dataset):
    def __init__(self, samples_root, ann_path):
        self.samples_root = samples_root
        with open(ann_path, 'r') as f:
            self.annotations = json.load(f)

        self.samples = []
        for vid, frames in self.annotations.items():
            for frame_id, bbox in frames.items():
                sample_file = os.path.join(samples_root, f"{vid}_f{frame_id}.npz")
                if os.path.exists(sample_file):
                    self.samples.append({
                        "sample_file": sample_file,
                        "bbox": bbox
                    })
        print(f"[Dataset] Loaded {len(self.samples)} samples")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        entry = self.samples[idx]
        data = np.load(entry["sample_file"])
        template = torch.tensor(data["template"], dtype=torch.float32)
        search = torch.tensor(data["search"], dtype=torch.float32)
        bbox = torch.tensor(entry["bbox"], dtype=torch.float32)
        return {"template": template, "search": search, "bbox": bbox}


# -------------------- Seed --------------------
def seed_torch(seed=0):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def is_dist_avail_and_initialized():
    """Kiá»ƒm tra xem PyTorch distributed cÃ³ kháº£ dá»¥ng vÃ  Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o chÆ°a."""
    if not torch.distributed.is_available():
        return False
    if not torch.distributed.is_initialized():
        return False
    return True


def build_data_loader(samples_root, ann_path, batch_size, num_workers):
    dataset = ProcessedDataset(samples_root, ann_path)

    sampler = None
    # âœ… Chá»‰ sá»­ dá»¥ng DistributedSampler náº¿u Ä‘Ã£ init dist vÃ  cÃ³ >1 GPU
    if is_dist_avail_and_initialized():
        try:
            world_size = get_world_size()
            if world_size > 1:
                sampler = DistributedSampler(dataset)
                print(f"ðŸ§© Using DistributedSampler with {world_size} processes")
        except Exception as e:
            print(f"âš ï¸ Distributed check failed, using single-process loader: {e}")

    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=True,
        sampler=sampler,
        shuffle=(sampler is None),  # âœ… shuffle náº¿u khÃ´ng dÃ¹ng sampler
        drop_last=True,
    )

    print(f"âœ… DataLoader built with {len(dataset)} samples, batch_size={batch_size}")
    return loader




# -------------------- Optimizer --------------------
def build_opt_lr(model, current_epoch=0):
    for param in model.backbone.parameters():
        param.requires_grad = False
    for m in model.backbone.modules():
        if isinstance(m, nn.BatchNorm2d):
            m.eval()

    trainable_params = [
        {"params": filter(lambda x: x.requires_grad, model.backbone.parameters()),
         "lr": cfg.BACKBONE.LAYERS_LR * cfg.TRAIN.BASE_LR},
        {"params": model.rpn_head.parameters(), "lr": cfg.TRAIN.BASE_LR},
    ]

    optimizer = torch.optim.SGD(
        trainable_params,
        momentum=cfg.TRAIN.MOMENTUM,
        weight_decay=cfg.TRAIN.WEIGHT_DECAY
    )
    lr_scheduler = build_lr_scheduler(optimizer, epochs=cfg.TRAIN.EPOCH)
    lr_scheduler.step(cfg.TRAIN.START_EPOCH)
    return optimizer, lr_scheduler


# -------------------- Training loop --------------------
def train(train_loader, model, optimizer, lr_scheduler, tb_writer):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    model.train()

    rank = get_rank()
    world_size = get_world_size()
    num_per_epoch = len(train_loader.dataset) // (cfg.TRAIN.BATCH_SIZE * world_size)
    start_epoch = cfg.TRAIN.START_EPOCH
    epoch = start_epoch
    average_meter = AverageMeter()

    os.makedirs(cfg.TRAIN.SNAPSHOT_DIR, exist_ok=True)

    for idx, data in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}/{cfg.TRAIN.EPOCH}")):
        data = {k: v.to(device, non_blocking=True) for k, v in data.items()}
        outputs = model(data)
        loss = outputs["total_loss"]

        optimizer.zero_grad()
        loss.backward()
        reduce_gradients(model)
        torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.TRAIN.GRAD_CLIP)
        optimizer.step()

        # Logging
        batch_info = {k: average_reduce(v.item()) for k, v in outputs.items()}
        average_meter.update(**batch_info)

        if rank == 0:
            for k, v in batch_info.items():
                tb_writer.add_scalar(k, v, idx)

        # Save checkpoints at each epoch
        if (idx + 1) % num_per_epoch == 0:
            epoch += 1
            if get_rank() == 0:
                ckpt = {
                    "epoch": epoch,
                    "state_dict": model.module.state_dict(),
                    "optimizer": optimizer.state_dict(),
                }
                torch.save(ckpt, os.path.join(cfg.TRAIN.SNAPSHOT_DIR, f"checkpoint_e{epoch}.pth"))
            if epoch >= cfg.TRAIN.EPOCH:
                break

def main():
    # -------------------- Parse config path --------------------
    parser = argparse.ArgumentParser(description="Train SiamRPN model")
    parser.add_argument("--cfg", type=str, required=True, help="Path to config.yaml")
    args = parser.parse_args()

    # -------------------- Kiá»ƒm tra GPU --------------------
    has_gpu = torch.cuda.is_available() and torch.cuda.device_count() > 0
    if has_gpu:
        rank, world_size = dist_init()
        device = torch.device("cuda")
        print(f"ðŸ”¥ Using {torch.cuda.device_count()} GPU(s)")
    else:
        rank, world_size = 0, 1
        device = torch.device("cpu")
        print("âš™ï¸  No GPU detected â€” training on CPU")

    seed_torch(42)

    # -------------------- Äá»c file config Ä‘Ãºng Ä‘Æ°á»ng dáº«n --------------------
    print(f"ðŸ“‚ Loading config from: {args.cfg}")
    cfg.merge_from_file(args.cfg)

    # -------------------- Build model --------------------
    model = ModelBuilder().to(device).train()

    if cfg.BACKBONE.PRETRAINED:
        backbone_path = "/kaggle/input/mobilenetv2/model.pth"
        if os.path.exists(backbone_path):
            load_pretrain(model.backbone, backbone_path)
        else:
            print("âš ï¸  Pretrained backbone not found")

    # -------------------- DataLoader --------------------
    train_loader = build_data_loader(
        samples_root="/kaggle/input/training-data/processed_dataset/samples",
        ann_path="/kaggle/input/training-data/processed_dataset/annotations/annotations.json",
        batch_size=cfg.TRAIN.BATCH_SIZE,
        num_workers=cfg.TRAIN.NUM_WORKERS
    )

    optimizer, lr_scheduler = build_opt_lr(model, cfg.TRAIN.START_EPOCH)
    tb_writer = SummaryWriter(cfg.TRAIN.LOG_DIR)

    if has_gpu and world_size > 1:
        model = DistModule(model)
        print("âœ… Using distributed training")
    else:
        print("âœ… Using single-device training")

    train(train_loader, model, optimizer, lr_scheduler, tb_writer)


if __name__ == "__main__":
    main()
